replicaCount: 1

deployment:
  image:
    repository: quay.io/go-skynet/local-ai  # Example: "docker.io/myapp"
    tag: latest    
  env:
    threads: 4
    context_size: 512
  modelsPath: "/models"
  download_model:
    # To use cloud provided (eg AWS) image, provide it like: 1234356789.dkr.ecr.us-REGION-X.amazonaws.com/busybox
    image: busybox
  prompt_templates:
    # To use cloud provided (eg AWS) image, provide it like: 1234356789.dkr.ecr.us-REGION-X.amazonaws.com/busybox
    image: busybox
  pullPolicy: IfNotPresent
  imagePullSecrets: []
    # - name: secret-names
  
  ## Needed for GPU Nodes
  #runtimeClassName: gpu

resources:
  {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

livenessProbe:
  httpGet:
    path: /healthz
    port: http
readinessProbe:
  httpGet:
    path: /readyz
    port: http


# Prompt templates to include
# Note: the keys of this map will be the names of the prompt template files
promptTemplates:
  {}
  # ggml-gpt4all-j.tmpl: |
  #   The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.
  #   ### Prompt:
  #   {{.Input}}
  #   ### Response:

# Models to download at runtime
models:
  configs:
    {}
    # myModel:
    #   backend: llama
    #   context_size: 2048
    #   name: myModel
    #   parameters:
    #     model: some_file.gguf
    #     temperature: 0.1
    #     top_p: 0.95
    #   threads: 5
    #   f16: true
    #   gpu_layers: 33

  galleries:
    model-gallery: github:go-skynet/model-gallery/index.yaml
    huggingface: github:go-skynet/model-gallery/huggingface.yaml

  loadFromGalleries:
    []
    # - "model-gallery@lunademo"
    # - huggingface@thebloke__llama-2-7b-chat-gguf__llama-2-7b-chat.q4_0.gguf

  # Whether to force download models even if they already exist
  forceDownload: false

  # The list of URLs to download models from
  # Note: the name of the file will be the name of the loaded model
  list:
    # - url: "https://gpt4all.io/models/ggml-gpt4all-j.bin"
    #   basicAuth: base64EncodedCredentials

initContainers: []
# Example:
# - name: my-init-container
#   image: my-init-image
#   imagePullPolicy: IfNotPresent
#   command: ["/bin/sh", "-c", "echo init"]
#   volumeMounts:
#     - name: my-volume
#       mountPath: /path/to/mount

enableModelSyncronizationSidecar: true
sidecarContainers: []
# Example:
# - name: my-sidecar-container
#   image: my-sidecar-image
#   imagePullPolicy: IfNotPresent
#   ports:
#     - containerPort: 1234

# Persistent storage for models and prompt templates.
# PVC and HostPath are mutually exclusive. If both are enabled,
# PVC configuration takes precedence. If neither are enabled, ephemeral
# storage is used.
persistence:
  models: 
    enabled: true
    annotations: {}
    storageClass: hostPath
    accessModes: ReadWriteMany
    size: 10Gi
    globalMount: /models
  output:
    enabled: true
    annotations: {}
    storageClass: hostPath
    accessModes: ReadWriteMany
    size: 5Gi
    globalMount: /tmp/generated

service:
  type: ClusterIP
  # If deferring to an internal only load balancer
  # externalTrafficPolicy: Local
  port: 80
  annotations: {}
  # If using an AWS load balancer, you'll need to override the default 60s load balancer idle timeout
  # service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "1200"

ingress:
  enabled: false
  className: ""
  annotations:
    {}
    # nginx.ingress.kubernetes.io/proxy-body-size: "25m" # This value determines the maxmimum uploadable file size
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

rbac:
  create: true
  ## Use an existing ClusterRole/Role (depending on rbac.namespaced false/true)
  # useExistingRole: name-of-some-role
  # useExistingClusterRole: name-of-some-clusterRole
  namespaced: false
  extraRoleRules: []
  # - apiGroups: []
  #   resources: []
  #   verbs: []
  extraClusterRoleRules: []
  # - apiGroups: []
  #   resources: []
  #   verbs: []
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

nodeSelector: {}

tolerations: []

affinity: {}


