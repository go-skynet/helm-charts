deployment:
  main:
    enabled: true
    replicaCount: 1
      # command: [] # Override the docker command, defualt /build/entrypoint.sh
      # args: [] # Additional arguments to pass into the docker entrypoint, e.g. --p2p
    env:
      P2P_TOKEN: ""
  worker: # Only use workers when aiming for p2p configuration.
    enabled: false
    replicaCount: 1
      # command: []
      # args: []
    env:
      P2P_TOKEN: ""

  # Specify the runtime class for the deployments (e.g., "nvidia" for GPU support)
  runtimeClassName: ""
  image:
    repository: quay.io/go-skynet/local-ai
    tag: latest    # Use the appropriate tag. E.g. for cpu only: "latest-cpu"
  pullPolicy: IfNotPresent
  # Path where models are mounted
  modelsPath: "/models"
  # Image pull secrets for private registries
  imagePullSecrets: []
  prompt_templates:
    # To use cloud provided (eg AWS) image, provide it like: 1234356789.dkr.ecr.us-REGION-X.amazonaws.com/busybox
    image: busybox

  # # Inject Secrets into Environment:
  secretEnv: []
  # - name: HF_TOKEN
  #   valueFrom:
  #     secretKeyRef:
  #       name: some-secret
  #       key: hf-token

# Model configurations
modelsConfigs: 
# Example:
  phi-2: |
    name: phi-2
    context_size: 2048
    f16: true
    mmap: true
    trimsuffix:
    - "\n"
    parameters:
      model: phi-2.Q8_0.gguf
      temperature: 0.2
      top_k: 40
      top_p: 0.95
      seed: -1
    template:
      chat: &template |-
        Instruct: {{.Input}}
        Output:
      completion: *template

# Prompt templates to include
# Note: the keys of this map will be the names of the prompt template files
promptTemplates: {}
# Example:
#   ggml-gpt4all-j.tmpl: |
#     The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.
#     ### Prompt:
#     {{.Input}}
#     ### Response:

resources:
# We usually recommend not to specify default resources and to leave this as a conscious
# choice for the user. This also increases chances charts run on environments with little
# resources, such as Minikube. If you do want to specify resources, uncomment the following
# lines, adjust them as necessary, and remove the curly braces after 'resources:'.
# Example:
#   limits:
#     cpu: 100m
#     memory: 128Mi
#   requests:
#     cpu: 100m
#     memory: 128Mi

initContainers: []
# Example:
# - name: my-init-container
#   image: my-init-image
#   imagePullPolicy: IfNotPresent
#   command: ["/bin/sh", "-c", "echo init"]
#   volumeMounts:
#     - name: my-volume
#       mountPath: /path/to/mount

sidecarContainers: []
# Example:
# - name: my-sidecar-container
#   image: my-sidecar-image
#   imagePullPolicy: IfNotPresent
#   ports:
#     - containerPort: 1234

persistence:
  main:
    models:
      enabled: true
      annotations: {}
      storageClass: ""
      accessModes:
        - ReadWriteOnce
      size: 10Gi
      globalMount: /models
    output:
      enabled: true
      annotations: {}
      storageClass: ""  
      accessModes:
        - ReadWriteOnce
      size: 5Gi
      globalMount: /tmp/generated
  worker:
    models:
      enabled: true
      annotations: {}
      storageClass: ""
      accessModes:
        - ReadWriteMany
      size: 10Gi
      globalMount: /models
    output:
      enabled: true
      annotations: {}
      storageClass: ""
      accessModes:
        - ReadWriteMany
      size: 5Gi
      globalMount: /tmp/generated

service:
  # Service type, e.g., ClusterIP, LoadBalancer
  type: ClusterIP
  # If deferring to an internal only load balancer
  # externalTrafficPolicy: Local
  # Service port
  port: 80
  # Service node port (if using a node port service type)
  # nodePort: 
  # Service annotations
  annotations: {}
  # If using an AWS load balancer, you'll need to override the default 60s load balancer idle timeout
  # service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "1200"

ingress:
  # Enable ingress
  enabled: false
  # Ingress class name
  className: ""
  # Ingress annotations
  annotations: {}
    # nginx.ingress.kubernetes.io/proxy-body-size: "25m" # This value determines the maxmimum uploadable file size
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  # Ingress hosts
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  # Ingress TLS configuration
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

# Node selector for scheduling
nodeSelector: {}
# Using Node Feature Discovery and the correct operator (e.g. nvidia-operator or intel-gpu-plugin), you can schedule nodes
#   nvidia.com/gpu.present: 'true'

# Tolerations for scheduling
tolerations: []

# Affinity rules for scheduling
affinity: {}
# Example affinity that ensures no pods are scheduled on the same node:
#   podAntiAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#       - labelSelector:
#           matchExpressions:
#             - key: app.kubernetes.io/name
#               operator: In
#               values:
#                 - localai
#         topologyKey: kubernetes.io/hostname

